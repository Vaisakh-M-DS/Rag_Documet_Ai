RAG Chatbot â€“ Local LLM (Qwen 2.5)

A Retrieval-Augmented Generation (RAG) chatbot that answers questions strictly based on uploaded documents, powered by Qwen 2.5 running locally via Ollama.
No external APIs. No data leakage.

ğŸš€ Features

ğŸ“‚ Document-based Question Answering

ğŸ” Semantic search using embeddings

ğŸ§  Local LLM inference with Qwen 2.5

âŒ Zero hallucination (document-only answers)

ğŸ” Rebuildable vector index

ğŸŒ Simple web UI

ğŸ—ï¸ Architecture
User Query
    â†“
Embedding Model
    â†“
FAISS Vector Store
    â†“
Top-K Relevant Chunks
    â†“
Qwen 2.5 (Local via Ollama)
    â†“
Final Answer

ğŸ§© Tech Stack
Backend

Python

Flask

FAISS

Custom RAG pipeline

Ollama (Qwen 2.5 â€“ Local LLM)

Frontend

React (Vite)

HTML / CSS / JavaScript

Models

LLM: Qwen 2.5 (Local)

Embeddings: Sentence Transformers / Ollama embeddings

ğŸ“ Project Structure
rag-chatbot/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ rag.py
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ documents/
â”‚   â””â”€â”€ vector_store/
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Setup & Run
1ï¸âƒ£ Install Ollama & Qwen 2.5
ollama pull qwen2.5


Verify:

ollama list

2ï¸âƒ£ Backend
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python app.py

3ï¸âƒ£ Frontend
cd frontend
npm install
npm run dev

ğŸ” How It Works

Documents are split into small chunks

Each chunk is embedded and stored in FAISS

User submits a query

Top-K relevant chunks are retrieved

Qwen 2.5 generates an answer using only retrieved context

If no relevant context is found, the system responds:

No relevant information found in the document.
